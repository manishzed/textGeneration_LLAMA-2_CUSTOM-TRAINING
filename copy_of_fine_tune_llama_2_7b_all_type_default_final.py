# -*- coding: utf-8 -*-
"""Copy of Fine_tune_Llama_2_7b_all_type_default_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14tU557K-nW6EI36CetJYtYzQeMkKfCwD

#**Step 1: Install All the Required Packages**
"""

!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7

"""#**Step 2: Import All the Required Libraries**"""

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, PeftModel
from trl import SFTTrainer

#from google.colab import drive
#drive.mount('/content/drive')

"""#We will reformat our instruction dataset to follow Llama 2’s template.

We have used Alpaca format introduced by Stanford to prepare dataset.

For more info visit this link : https://github.com/tatsu-lab/stanford_alpaca

#To drastically reduce the VRAM usage, we must fine-tune the model in 4-bit precision, which is why we’ll use QLoRA here.

#**Step 3**

QLoRA will use a rank of 64 with a scaling parameter of 16. We’ll load the Llama 2 model directly in 4-bit precision using the NF4 type and train it for one epoch
"""

# The model that you want to train from the Hugging Face hub
model_name = "NousResearch/Llama-2-7b-chat-hf"

# The instruction dataset to use
#dataset_name = "ekshat/text-2-sql-with-context"

# Fine-tuned model name
new_model = "/content/Llama2"

################################################################################
# QLoRA parameters
################################################################################

# LoRA attention dimension
lora_r = 64

# Alpha parameter for LoRA scaling
lora_alpha = 16

# Dropout probability for LoRA layers
lora_dropout = 0.1

################################################################################
# bitsandbytes parameters
################################################################################

# Activate 4-bit precision base model loading
use_4bit = True

# Compute dtype for 4-bit base models
bnb_4bit_compute_dtype = "float16"

# Quantization type (fp4 or nf4)
bnb_4bit_quant_type = "nf4"

# Activate nested quantization for 4-bit base models (double quantization)
use_nested_quant = False

################################################################################
# TrainingArguments parameters
################################################################################

# Output directory where the model predictions and checkpoints will be stored
output_dir = "/content/results"

# Number of training epochs
num_train_epochs = 1

# Enable fp16/bf16 training (set bf16 to True with an A100)
fp16 = False
bf16 = False

# Batch size per GPU for training
per_device_train_batch_size = 8

# Batch size per GPU for evaluation
per_device_eval_batch_size = 4

# Number of update steps to accumulate the gradients for
gradient_accumulation_steps = 1

# Enable gradient checkpointing
gradient_checkpointing = True

# Maximum gradient normal (gradient clipping)
max_grad_norm = 0.3

# Initial learning rate (AdamW optimizer)
learning_rate = 2e-4

# Weight decay to apply to all layers except bias/LayerNorm weights
weight_decay = 0.001

# Optimizer to use
optim = "paged_adamw_32bit"

# Learning rate schedule
lr_scheduler_type = "cosine"

# Number of training steps (overrides num_train_epochs)
max_steps = -1

# Ratio of steps for a linear warmup (from 0 to learning rate)
warmup_ratio = 0.03

# Group sequences into batches with same length
# Saves memory and speeds up training considerably
group_by_length = True

# Save checkpoint every X updates steps
save_steps = 0

# Log every X updates steps
logging_steps = 25

################################################################################
# SFT parameters
################################################################################

# Maximum sequence length to use
max_seq_length = None

# Pack multiple short examples in the same input sequence to increase efficiency
packing = False

# Load the entire model on the GPU 0
device_map = {"": 0}

"""#**Step 4:Load everything and start the fine-tuning process**

1. First of all, we want to load the dataset we defined. Here, our dataset is already preprocessed but, usually, this is where you would reformat the prompt, filter out bad text, combine multiple datasets, etc.


2. Then, we’re configuring bitsandbytes for 4-bit quantization.


3. Next, we're loading the Llama 2 model in 4-bit precision on a GPU with the corresponding tokenizer.


4. Finally, we're loading configurations for QLoRA, regular training parameters, and passing everything to the SFTTrainer. The training can finally start!
"""

import pandas as pd

df_train_ =pd.read_csv("/content/dataset_train.csv")
df_train_.head()

train_file = "/content/dataset_train.csv"
# Get the datasets

from datasets import load_dataset

data_files = {}
dataset_args = {}
validation_split_percentage = 5
extension = "csv"
data_files = {
    "train": train_file,
}

raw_datasets = load_dataset(
    extension,
    sep=";",
    data_files=data_files
)

raw_datasets["validation"] = load_dataset(
    extension,
    sep=";",
    data_files=data_files,
    split=f"train[:{validation_split_percentage}%]",
    **dataset_args,
)

raw_datasets["train"] = load_dataset(
    extension,
    sep=";",
    data_files=data_files,
    split=f"train[{validation_split_percentage}%:]",
    **dataset_args,
)

raw_datasets

raw_datasets['train']['text'][1:10]

raw_datasets['validation']['text'][1:10]

# Load dataset (you can process it here)
#from datasets import Dataset
#dataset = Dataset.load_from_disk(dataset_name)
#from datasets import load_dataset
#datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')
#dataset = load_dataset(dataset_name)

dataset =raw_datasets
dataset

dataset['train']['text']

#from sklearn.model_selection import train_test_split

#dataset = dataset['train'].train_test_split(test_size=0.08)

# Load tokenizer and model with QLoRA configuration
compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

# Check GPU compatibility with bfloat16
if compute_dtype == torch.float16 and use_4bit:
    major, _ = torch.cuda.get_device_capability()
    if major >= 8:
        print("=" * 80)
        print("Your GPU supports bfloat16: accelerate training with bf16=True")
        print("=" * 80)

# Load base model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map=device_map
)
model.config.use_cache = False
model.config.pretraining_tp = 1

# Load LLaMA tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" # Fix weird overflow issue with fp16 training

# Load LoRA configuration
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    task_type="CAUSAL_LM",
)

# Set training parameters
training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    weight_decay=weight_decay,
    fp16=fp16,
    bf16=bf16,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    report_to="tensorboard"
)

# Set supervised fine-tuning parameters
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset['train'],
    eval_dataset=dataset['validation'],
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=packing,
)

# Train model
trainer.train()

# Save trained model
trainer.model.save_pretrained(new_model)

"""##**Step 5: Check the plots on tensorboard, as follows**"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir results/runs

"""#**Step 6:Use the text generation pipeline to ask questions like “What is a large language model?” Note that I’m formatting the input to match Llama 2’s prompt template.**"""

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# Ignore warnings
logging.set_verbosity(logging.CRITICAL)

prompt = "as a List the season above 241.0 that was handled by brad tanenbaum."
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=100)
result = pipe(prompt)
#print(result[0]['generated_text'])

print(result[0]['generated_text'].split("<|endoftext|>")[0])

# Empty VRAM
del model
del pipe
del trainer
import gc
gc.collect()
gc.collect()

"""#**Step 7: Store New Llama2 Model (Llama-2-7b-chat-finetune)**

How can we store our new Llama-2-7b-chat-finetune model now? We need to merge the weights from LoRA with the base model. Unfortunately, as far as I know, there is no straightforward way to do it: we need to reload the base model in FP16 precision and use the peft library to merge everything.
"""

# Reload model in FP16 and merge it with LoRA weights
# The model that you want to train from the Hugging Face hub
model_name = "NousResearch/Llama-2-7b-chat-hf"
# Fine-tuned model name
new_model = "/content/Llama2_text-to-sql"

base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map=device_map,
)

model = PeftModel.from_pretrained(base_model, new_model)
model = model.merge_and_unload()

# Reload tokenizer to save it
# Load LLaMA tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" # Fix weird overflow issue with fp16 training

# Ignore warnings
logging.set_verbosity(logging.CRITICAL)

prompt = "as a What's the lowest bronze with a 6 rank, smaller than 5 gold, and a total of more than 1?"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)
result = pipe(prompt)
#print(result[0]['generated_text'])
print(result[0]['generated_text'].split("<|endoftext|>")[0])

# Empty VRAM
del model
del pipe
import gc
gc.collect()
gc.collect()

"""#**Step 8: Push Model to Hugging Face Hub**

Our weights are merged and we reloaded the tokenizer. We can now push everything to the Hugging Face Hub to save our model.
"""

import locale
locale.getpreferredencoding = lambda: "UTF-8"

!huggingface-cli login

model.push_to_hub("ekshat/Llama-2-7b-chat-finetune-for-text2sql", check_pr=True)

tokenizer.push_to_hub("ekshat/Llama-2-7b-chat-finetune-for-text2sql",check_pr=True)













!pip install -q -U accelerate==0.23.0 peft==0.5.0 bitsandbytes==0.41.1 transformers==4.31 trl==0.7.2

import numpy as np
import pandas as pd
import os
from tqdm import tqdm
import bitsandbytes as bnb
import torch
import torch.nn as nn
import transformers
from datasets import Dataset
from peft import LoraConfig, PeftConfig
from trl import SFTTrainer
from transformers import (AutoModelForCausalLM,
                          AutoTokenizer,
                          BitsAndBytesConfig,
                          TrainingArguments,
                          pipeline,
                          logging)
from sklearn.metrics import (accuracy_score,
                             classification_report,
                             confusion_matrix)
from sklearn.model_selection import train_test_split

import pandas as pd

filename = "/content/all-data.csv"

df = pd.read_csv(filename,
                 names=["sentiment", "text"],
                 encoding="utf-8", encoding_errors="replace")

df.head()

X_train = list()
X_test = list()
for sentiment in ["positive", "neutral", "negative"]:
    print("11111111111", sentiment, "......", df[df.sentiment==sentiment])
    train, test  = train_test_split(df[df.sentiment==sentiment],
                                    train_size=300,
                                    test_size=300,
                                    random_state=42)
    X_train.append(train)
    X_test.append(test)

X_train = pd.concat(X_train).sample(frac=1, random_state=10)
X_test = pd.concat(X_test)

X_train, X_test

eval_idx = [idx for idx in df.index if idx not in list(train.index) + list(test.index)]
X_eval = df[df.index.isin(eval_idx)]
X_eval = (X_eval
          .groupby('sentiment', group_keys=False)
          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))
X_train = X_train.reset_index(drop=True)
X_train

def generate_prompt(data_point):
    return f"""
            Analyze the sentiment of the news headline enclosed in square brackets,
            determine if it is positive, neutral, or negative, and return the answer as
            the corresponding sentiment label "positive" or "neutral" or "negative".

            [{data_point["text"]}] = {data_point["sentiment"]}
            """.strip()
def generate_test_prompt(data_point):
    return f"""
            Analyze the sentiment of the news headline enclosed in square brackets,
            determine if it is positive, neutral, or negative, and return the answer as
            the corresponding sentiment label "positive" or "neutral" or "negative".

            [{data_point["text"]}] = """.strip()



X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1),
                       columns=["text"])
X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1),
                      columns=["text"])

y_true = X_test.sentiment
X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=["text"])

train_data = Dataset.from_pandas(X_train)
eval_data = Dataset.from_pandas(X_eval)

X_eval['text'][972]

X_train['text'][1]

y_true

X_test['text'][2]

def evaluate(y_true, y_pred):
    labels = ['positive', 'neutral', 'negative']
    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}
    def map_func(x):
        return mapping.get(x, 1)

    y_true = np.vectorize(map_func)(y_true)
    y_pred = np.vectorize(map_func)(y_pred)

    # Calculate accuracy
    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)
    print(f'Accuracy: {accuracy:.3f}')

    # Generate accuracy report
    unique_labels = set(y_true)  # Get unique labels

    for label in unique_labels:
        label_indices = [i for i in range(len(y_true))
                         if y_true[i] == label]
        label_y_true = [y_true[i] for i in label_indices]
        label_y_pred = [y_pred[i] for i in label_indices]
        accuracy = accuracy_score(label_y_true, label_y_pred)
        print(f'Accuracy for label {label}: {accuracy:.3f}')

    # Generate classification report
    class_report = classification_report(y_true=y_true, y_pred=y_pred)
    print('\nClassification Report:')
    print(class_report)

    # Generate confusion matrix
    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])
    print('\nConfusion Matrix:')
    print(conf_matrix)

model_name = "NousResearch/Llama-2-7b-hf"
#model_name="meta-llama/Llama-2-7b-hf"
compute_dtype = getattr(torch, "float16")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
)

model.config.use_cache = False
model.config.pretraining_tp = 1

tokenizer = AutoTokenizer.from_pretrained(model_name,
                                          trust_remote_code=True,
                                         )
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

def predict(test, model, tokenizer):
    y_pred = []
    for i in tqdm(range(len(X_test))):
        prompt = X_test.iloc[i]["text"]
        pipe = pipeline(task="text-generation",
                        model=model,
                        tokenizer=tokenizer,
                        max_new_tokens = 1,
                        temperature = 0.0,
                       )
        result = pipe(prompt)
        answer = result[0]['generated_text'].split("=")[-1]
        if "positive" in answer:
            y_pred.append("positive")
        elif "negative" in answer:
            y_pred.append("negative")
        elif "neutral" in answer:
            y_pred.append("neutral")
        else:
            y_pred.append("none")
    return y_pred

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)

training_arguments = TrainingArguments(
    output_dir="logs",
    num_train_epochs=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8, # 4
    optim="paged_adamw_32bit",
    save_steps=0,
    logging_steps=25,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=True,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="cosine",
    report_to="tensorboard",
    evaluation_strategy="epoch"
)

trainer = SFTTrainer(
    model=model,
    train_dataset=train_data,
    eval_dataset=eval_data,
    peft_config=peft_config,
    dataset_text_field="text",
    tokenizer=tokenizer,
    args=training_arguments,
    packing=False,
    max_seq_length=1024,
)

# Train model
trainer.train()

# Save trained model
trainer.model.save_pretrained("trained-model")

y_pred = predict(test, model, tokenizer)
evaluate(y_true, y_pred)

evaluation = pd.DataFrame({'text': X_test["text"],
                           'y_true':y_true,
                           'y_pred': y_pred},
                         )
evaluation.to_csv("test_predictions.csv", index=False)

X_test['text'][2]

X_train['text'][1]

test1 ='Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label "positive" or "neutral" or "negative".\n\n            [Both operating profit and net sales for the 12-month period increased , respectively from EUR21 .5 m and EUR196 .1 m , as compared to 2005 .] = '
y_pred = []
prompt = test1
pipe = pipeline(task="text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens = 1,
                temperature = 0.0,
                )
result = pipe(prompt)
#answer = result[0]['generated_text'].split("=")[-1]
answer = result[0]['generated_text']
answer

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# Ruta del modelo guardado en el dataset de Kaggle
from peft import LoraConfig, PeftModel

device_map = {"": 0}
PEFT_MODEL = "/content/trained-model"
#model_name = "NousResearch/Llama-2-7b-hf"

# Cargar la configuración del modelo
config = PeftConfig.from_pretrained(PEFT_MODEL)

# Cargar el modelo
model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    low_cpu_mem_usage=True,
    return_dict=True,
    #quantization_config=bnb_config,
    device_map="auto",
    #trust_remote_code=True,
    torch_dtype=torch.float16,
)

# Cargar el tokenizador
tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)
tokenizer.pad_token = tokenizer.eos_token

# Cargar el modelo PEFT
load_model = PeftModel.from_pretrained(model, PEFT_MODEL)

test1 ='Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label "positive" or "neutral" or "negative".\n\n            [Both operating profit and net sales for the 12-month period increased , respectively from EUR21 .5 m and EUR196 .1 m , as compared to 2005 .] = '
y_pred = []
prompt = test1
pipe = pipeline(task="text-generation",
                model=load_model,
                tokenizer=tokenizer,
                max_new_tokens = 1,
                temperature = 0.0,
                )
result = pipe(prompt)
#answer = result[0]['generated_text'].split("=")[-1]
answer = result[0]['generated_text']
answer



















#!rm -rf /content/logs_keywords

import numpy as np
import pandas as pd
import os
from tqdm import tqdm
import bitsandbytes as bnb
import torch
import torch.nn as nn
import transformers
from datasets import Dataset
from peft import LoraConfig, PeftConfig
from trl import SFTTrainer
from transformers import (AutoModelForCausalLM,
                          AutoTokenizer,
                          BitsAndBytesConfig,
                          TrainingArguments,
                          pipeline,
                          logging)
from sklearn.metrics import (accuracy_score,
                             classification_report,
                             confusion_matrix)
from sklearn.model_selection import train_test_split

df =pd.read_csv("/content/dataset_all.csv")
df =df.iloc[0:1500,:]
df.head()
print(len(df))

from sklearn.model_selection import train_test_split
# Split the data into training, testing, and validation sets
# First, split into train and temp sets (80% train and 20% temp)
X_train, X_temp = train_test_split(df, test_size=0.2, random_state=42)

# Then, split the temp set into test and validation sets (50% test and 50% validation)
X_test, X_eval = train_test_split(X_temp, test_size=0.5, random_state=42)

X_train

def generate_prompt(data_point):
    return f"""
            Analyze the keywords enclosed in square brackets,
            determine description if it is belong to keywords, and return the answer as
            the corresponding description.

            [{data_point["keywords"]}] = {data_point["description"]}
            """.strip()
def generate_test_prompt(data_point):
    return f"""
            Analyze the keywords enclosed in square brackets,
            determine description if it is belong to keywords, and return the answer as
            the corresponding description.

            [{data_point["keywords"]}] = """.strip()



X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1),
                       columns=["text"])
X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1),
                      columns=["text"])

y_true = X_test.description
X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=["text"])

train_data = Dataset.from_pandas(X_train)
eval_data = Dataset.from_pandas(X_eval)

X_train['text'][1]

X_test['text'][1161]

y_true[1161]

X_eval['text'][590]

#!rm -rf /content/logs_keywords

model_name = "NousResearch/Llama-2-7b-hf"
#model_name="meta-llama/Llama-2-7b-hf"
compute_dtype = getattr(torch, "float16")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
)

model.config.use_cache = False
model.config.pretraining_tp = 1

tokenizer = AutoTokenizer.from_pretrained(model_name,
                                          trust_remote_code=True,
                                         )
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)

training_arguments = TrainingArguments(
    output_dir="results_keywords",
    num_train_epochs=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8, # 4
    optim="paged_adamw_32bit",
    save_steps=0,
    logging_steps=25,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=True,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="cosine",
    report_to="tensorboard",
    evaluation_strategy="epoch"
)

trainer = SFTTrainer(
    model=model,
    train_dataset=train_data,
    eval_dataset=eval_data,
    peft_config=peft_config,
    dataset_text_field="text",
    tokenizer=tokenizer,
    args=training_arguments,
    packing=False,
    max_seq_length=1024,
)

# Train model
trainer.train()

# Save trained model
trainer.model.save_pretrained("Llama2_keywords")

#'Analyze the keywords enclosed in square brackets, determine description if it is belong to keywords, and return the answer as the corresponding description. [database administrator] = designed and built an HA DBaaS platform using MySQL and Galera.'
test1 ='Analyze the keywords enclosed in square brackets,\n            determine description if it is belong to keywords, and return the answer as\n            the corresponding description.\n\n            [database administrator] ='
test2 ='Analyze the keywords enclosed in square brackets,\n            determine description if it is belong to keywords, and return the answer as\n            the corresponding description.\n\n            [backend developer] ='
test3 ='Analyze the keywords enclosed in square brackets,\n            determine description if it is belong to keywords, and return the answer as\n            the corresponding description.\n\n            [frontend developer] ='
test4 ='Analyze the keywords enclosed in square brackets,\n            determine description if it is belong to keywords, and return the answer as\n            the corresponding description.\n\n            [python developer] ='

prompt = test4
pipe = pipeline(task="text-generation",
                model=model,
                tokenizer=tokenizer,
                #max_length =20,
                max_new_tokens =20,
                temperature = 0.0,
                )
result = pipe(prompt)
#answer1 = " ".join(result[0]['generated_text'].split("=")[:-1]).strip()
#print(answer1)
answer2 = result[0]['generated_text']
print(answer2)

answerx = " ".join(result[0]['generated_text'].split(".")[:-1])
answerx

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# Ruta del modelo guardado en el dataset de Kaggle
from peft import LoraConfig, PeftModel

device_map = {"": 0}
PEFT_MODEL = "/content/Llama2_keywords"
#model_name = "NousResearch/Llama-2-7b-hf"

# Cargar la configuración del modelo
config = PeftConfig.from_pretrained(PEFT_MODEL)

# Cargar el modelo
model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    low_cpu_mem_usage=True,
    return_dict=True,
    #quantization_config=bnb_config,
    device_map="auto",
    #trust_remote_code=True,
    torch_dtype=torch.float16,
)

# Cargar el tokenizador
tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)
tokenizer.pad_token = tokenizer.eos_token

# Cargar el modelo PEFT
load_model = PeftModel.from_pretrained(model, PEFT_MODEL)

test1 ='Analyze the keywords enclosed in square brackets,\n            determine description if it is belong to keywords, and return the answer as\n            the corresponding description.\n\n            [database administrator] ='
test2 ='Analyze the keywords enclosed in square brackets,\n            determine description if it is belong to keywords, and return the answer as\n            the corresponding description.\n\n            [backend developer] ='
test3 ='Analyze the keywords enclosed in square brackets,\n            determine description if it is belong to keywords, and return the answer as\n            the corresponding description.\n\n            [frontend developer] ='
test4 ='Analyze the keywords enclosed in square brackets,\n            determine description if it is belong to keywords, and return the answer as\n            the corresponding description.\n\n            [python developer] ='
test5='Analyze the keywords enclosed in square brackets,\n            determine description if it is belong to keywords, and return the answer as\n            the corresponding description.\n\n            [java developer] ='
test6='Analyze the keywords enclosed in square brackets,\n            determine description if it is belong to keywords, and return the answer as\n            the corresponding description.\n\n            [c# developer] ='
prompt_test = test1
pipe_test = pipeline(task="text-generation",
                model=load_model,
                tokenizer=tokenizer,
                max_new_tokens = 25,
                #temperature = 0.1,
                #max_length =50,
                )
result_test = pipe_test(prompt_test)
#answer = result[0]['generated_text'].split("=")[-1]
answer_test = result_test[0]['generated_text']
answer_test

answery = " ".join(result_test[0]['generated_text'].split(".")[:-1])
answery

l









import numpy as np
import pandas as pd
import os
from tqdm import tqdm
import bitsandbytes as bnb
import torch
import torch.nn as nn
import transformers
from datasets import Dataset
from peft import LoraConfig, PeftConfig
from trl import SFTTrainer
from transformers import (AutoModelForCausalLM,
                          AutoTokenizer,
                          BitsAndBytesConfig,
                          TrainingArguments,
                          pipeline,
                          logging)
from sklearn.metrics import (accuracy_score,
                             classification_report,
                             confusion_matrix)
from sklearn.model_selection import train_test_split

from datasets import load_dataset
#datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')
datasets = load_dataset("wikisql")
datasets

import pandas as pd
from datasets import load_dataset

# Load the 'wikisql' dataset
dataset = load_dataset("wikisql")

# Combine the dataset into a Pandas DataFrame
df = pd.DataFrame({
    'question': dataset['train']['question'],
    'sql': dataset['train']['sql'],
})

# Print the first few rows of the DataFrame
print(df.head())

df['sql_cleaned']=df['sql'].apply(lambda x:x['human_readable'])
df.head()

df['sql_cleaned'][1]

#df =pd.read_csv("/content/dataset_all.csv")
df=df[["question","sql_cleaned"]]
df =df.iloc[0:1500,:]
print(len(df))
df.head()

from sklearn.model_selection import train_test_split
# Split the data into training, testing, and validation sets
# First, split into train and temp sets (80% train and 20% temp)
X_train, X_temp = train_test_split(df, test_size=0.2, random_state=42)

# Then, split the temp set into test and validation sets (50% test and 50% validation)
X_test, X_eval = train_test_split(X_temp, test_size=0.5, random_state=42)

X_train

def generate_prompt(data_point):
    return f"""
            Analyze the sql question enclosed in square brackets,
            determine sql sytax if it is belong to sql, and return the answer as
            the corresponding sql sytax.

            [{data_point["question"]}] = {data_point["sql_cleaned"]}
            """.strip()
def generate_test_prompt(data_point):
    return f"""
            Analyze the sql question enclosed in square brackets,
            determine sql sytax if it is belong to sql, and return the answer as
            the corresponding sql sytax.

            [{data_point["question"]}] = """.strip()



X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1),
                       columns=["text"])
X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1),
                      columns=["text"])

y_true = X_test.sql_cleaned
X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=["text"])

train_data = Dataset.from_pandas(X_train)
eval_data = Dataset.from_pandas(X_eval)

X_train['text'][390]

X_eval['text'][464]

X_test['text'][899]

y_true[899]

model_name = "NousResearch/Llama-2-7b-hf"
#model_name="meta-llama/Llama-2-7b-hf"
compute_dtype = getattr(torch, "float16")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
)

model.config.use_cache = False
model.config.pretraining_tp = 1

tokenizer = AutoTokenizer.from_pretrained(model_name,
                                          trust_remote_code=True,
                                         )
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)

training_arguments = TrainingArguments(
    output_dir="results_sql",
    num_train_epochs=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8, # 4
    optim="paged_adamw_32bit",
    save_steps=0,
    logging_steps=25,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=True,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="cosine",
    report_to="tensorboard",
    evaluation_strategy="epoch"
)

trainer = SFTTrainer(
    model=model,
    train_dataset=train_data,
    eval_dataset=eval_data,
    peft_config=peft_config,
    dataset_text_field="text",
    tokenizer=tokenizer,
    args=training_arguments,
    packing=False,
    max_seq_length=1024,
)

# Train model
trainer.train()

# Save trained model
trainer.model.save_pretrained("Llama2_sql")

#'Analyze the keywords enclosed in square brackets, determine description if it is belong to keywords, and return the answer as the corresponding description. [database administrator] = designed and built an HA DBaaS platform using MySQL and Galera.'
test1 ='Analyze the sql question enclosed in square brackets,\n            determine sql sytax if it is belong to sql, and return the answer as\n            the corresponding sql sytax.\n\n            [How many records are there at the War Memorial Stadium?] ='
test2 ='Analyze the sql question enclosed in square brackets,\n            determine sql sytax if it is belong to sql, and return the answer as\n            the corresponding sql sytax.\n\n            [The Dijon-prenois had how many fastest laps?] ='
test3 ='Analyze the sql question enclosed in square brackets,\n            determine sql sytax if it is belong to sql, and return the answer as\n            the corresponding sql sytax.\n\n            [In which stadium is the week 5 game played?] ='
test4 ="Analyze the sql question enclosed in square brackets,\n            determine sql sytax if it is belong to sql, and return the answer as\n            the corresponding sql sytax.\n\n            [What are the chances that player 2 wins if player 1's choice is BB R?] ="

prompt = test3
pipe = pipeline(task="text-generation",
                model=model,
                tokenizer=tokenizer,
                #max_length =20,
                max_new_tokens =25,
                temperature = 0.0,
                )
result = pipe(prompt)
#answer1 = " ".join(result[0]['generated_text'].split("=")[:-1]).strip()
#print(answer1)
answer2 = result[0]['generated_text']
print(answer2)

answerx = " ".join(result[0]['generated_text'].split("[")[:-1])
answerx

#testing and loading model

import torch, gc
gc.collect()
torch.cuda.empty_cache()

# Ruta del modelo guardado en el dataset de Kaggle
from peft import LoraConfig, PeftModel

device_map = {"": 0}
PEFT_MODEL = "/content/Llama2_sql"
#model_name = "NousResearch/Llama-2-7b-hf"

# Cargar la configuración del modelo
config = PeftConfig.from_pretrained(PEFT_MODEL)

# Cargar el modelo
model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    low_cpu_mem_usage=True,
    return_dict=True,
    #quantization_config=bnb_config,
    device_map="auto",
    #trust_remote_code=True,
    torch_dtype=torch.float16,
)

# Cargar el tokenizador
tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)
tokenizer.pad_token = tokenizer.eos_token

# Cargar el modelo PEFT
load_model = PeftModel.from_pretrained(model, PEFT_MODEL)

test1 ='Analyze the sql question enclosed in square brackets,\n            determine sql sytax if it is belong to sql, and return the answer as\n            the corresponding sql sytax.\n\n            [How many records are there at the War Memorial Stadium?] ='
test2 ='Analyze the sql question enclosed in square brackets,\n            determine sql sytax if it is belong to sql, and return the answer as\n            the corresponding sql sytax.\n\n            [The Dijon-prenois had how many fastest laps?] ='
test3 ='Analyze the sql question enclosed in square brackets,\n            determine sql sytax if it is belong to sql, and return the answer as\n            the corresponding sql sytax.\n\n            [In which stadium is the week 5 game played?] ='
test4 ="Analyze the sql question enclosed in square brackets,\n            determine sql sytax if it is belong to sql, and return the answer as\n            the corresponding sql sytax.\n\n            [What are the chances that player 2 wins if player 1's choice is BB R?] ="


prompt_test = test1
pipe_test = pipeline(task="text-generation",
                model=load_model,
                tokenizer=tokenizer,
                #max_length =20,
                max_new_tokens =25,
                temperature = 0.0,
                )
result_test = pipe_test(prompt_test)
#answer = result[0]['generated_text'].split("=")[-1]
answer_test = result_test[0]['generated_text']
answer_test

answery = " ".join(result_test[0]['generated_text'].split("[")[:-1])
answery

